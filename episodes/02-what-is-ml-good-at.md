---
title: AI@SI_03_what-is-ml-good-at
teaching: 30
exercises: 10
questions:
- "What areas of machine learning are well-suited for domain experts?"
objectives:
- "First learning objective. (FIXME)"
keypoints:
- "First key point. Brief Answer to questions. (FIXME)"
slideOptions:
  theme: solarized
  progress: true
---

## What is Machine Learning good at?

---

### Feature engineering

Additional features can be derived from single columns -- like dates. **Feature engineering** is the process of creating featues from existing data, and it is greatly benefited by domain expertise.

---

### Feature engineering

[ToDo: diagram showing single date field exploded into several columns]

---

### Deep Learning

Feature engineering is not as necessary for deep learning models (for Computer Vision or Natural Language Processing).

---

### Deep Learning

Tell the ImageNet story?

---

### Transfer Learning

Thanks to emphasis on reproducibility and race to achieve State of the Art for a task, there are a lot of open source models trained on extremely large datasets.

Examples: ImageNet computer vision models; NLP transformers models built on Wikpedia or Common Crawl

---

### Transfer Learning

[diagram of huge CNN]

In most computer vision classification models, the input image goes through hundreds of layers, which results in a "feature vector" (sometimes referred to as an embedding). This feature vector is then classified into a limited number of classes.

If you want to use your own classes, you can take those vectors that come from a general case model, and re-train the classification step.

You can also "fine-tune" the weights in the previous layers of the model.

---

### Unsupervised Learning with Feature Vectors

You can take feature vectors generated by pre-built models, and calculate distances between them, and produce clusterings.

Show PixPlot example or Smithsonian paintings at https://github.com/sidatasciencelab/siopenaccess

---

### The ML community is good at open source

You can build machine learning models with the same tools that Google (TensorFlow) and Facebook (PyTorch) use internally.

Downside: they make their tools open source so that developers can make improvements. You cannot make your own Google search tool or Faceboook facial recognition tool, because they are built on mountains of propietary data.

---

### Downside of Complex Models

The more parameters your model has, the harder it becomes to explain why the model made a prediction.

Some state of the art NLP models have billions of parameters!

---

### Downside of Complex Models

Show a simple decision tree, and contrast with a heat-mapped image classification task.

Can also use sentiment analysis example.

---

### Downside of Complex Models

Even though fine-tuning is possible on large models, their datasets are made up of modern photos and a web text. A lot of historical or niche topics do not perform well.