<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Intro to AI for GLAM: Understanding and managing bias</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png"><link rel="manifest" href="favicons/incubator/site.webmanifest"><link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Lesson Description" src="assets/images/incubator-logo.svg"><span class="badge text-bg-info">
          <abbr title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#polishing-beta-stage" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
              Beta
            </a>
            <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/05-managing-data-bias.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Intro to AI for GLAM
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Intro to AI for GLAM
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><li><a class="dropdown-item" href="discuss.html">Discussion</a></li><li><a class="dropdown-item" href="reference.html">Course Learner Profiles</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Intro to AI for GLAM
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 100%" class="percentage">
    100%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 100%" aria-valuenow="100" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/05-managing-data-bias.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-welcome.html">1. Welcome</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-AI-in-a-Nutshell.html">2. Artificial Intelligence (AI) and Machine Learning (ML) in a nutshell</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-AI-ML-concepts.html">3. Machine Learning Modelling Concepts</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-what-is-ml-good-at.html">4. What is Machine Learning good at?</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        5. Understanding and managing bias
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#bias-in-machine-learning">Bias in machine learning</a></li>
<li><a href="#common-bias-types-in-machine-learning">Common bias types in machine learning</a></li>
<li><a href="#when-might-human-bias-enter-a-machine-learning-pipeline">When might human bias enter a machine learning pipeline?</a></li>
<li><a href="#how-can-glam-staff-help-manage-bias-in-machine-learning-approaches">How can GLAM staff help manage bias in machine learning
approaches?</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-applying-machine-learning.html">6. Applying Machine Learning</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-ecosystem.html">7. The Machine Learning ecosystem</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="discuss.html">Discussion</a></li><li><a href="reference.html">Course Learner Profiles</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="04-what-is-ml-good-at.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="06-applying-machine-learning.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="04-what-is-ml-good-at.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: What is Machine
        </a>
        <a class="chapter-link float-end" href="06-applying-machine-learning.html" rel="next">
          Next: Applying Machine...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Understanding and managing bias</h1>
        <p>Last updated on 2024-11-14 |

        <a href="https://github.com/carpentries-incubator/machine-learning-librarians-archivists/edit/main/episodes/05-managing-data-bias.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What are common types of bias and their effect in machine
learning?</li>
<li>At what points can bias enter the machine learning pipeline?</li>
<li>Can we manage bias? Some lessons from GLAM</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Define bias in the context of machine learning</li>
<li>Identify common types of bias and how and at what stages these may
impact model predictions</li>
<li>Give examples of a range of bias mitigation strategies available to
GLAM staff</li>
</ul></div>
</div>
</div>
</div>
</div>
<p>FIXME</p>
<section><h2 class="section-heading" id="bias-in-machine-learning">Bias in machine learning<a class="anchor" aria-label="anchor" href="#bias-in-machine-learning"></a></h2>
<hr class="half-width"><p>Though AI and machine learning (ML) systems may appear to us as
objective, dealing solely in facts and numbers, free from troublesome
human proclivities in their decision making, there are abundant
opportunities for human bias to enter ML systems at all stages of the
pipeline.</p>
<p>Human bias can have a broad and complex range of effects on the
classification and predictions of models, with consequences of varying
degrees. Bias in AI can be understood in most general terms as an error
where incorrect assumptions lead to systematically prejudiced
results.</p>
<p>When we typically hear about bias in AI these days, particularly in
the news, it is most often presented and understood in the context of
societal prejudice and discrimination, where human prejudice in the
development and application of algorithms results in the perpetuation of
unfairness, inequities, and stereotypes of the real world.</p>
<p><a href="https://en.wikipedia.org/wiki/Predictive_policing" class="external-link">Predictive
policing systems</a> are a canonical illustration of this. Such systems
are being relied upon as tools for courts to use in sentencing, aiming
to predict the likelihood of defendants committing a future crime.
Analysis by ProPublica in 2016 of one system used widely throughout the
United States, Correctional Offender Management Profiling for
Alternative Sanctions (COMPAS) tool, uncovered significant racial
disparity between the system predictions for white and black defendants.
The COMPAS tool assigns scores from 1 to 10 to a defendent based on 100
or so factors such as age, sex and criminal history (although notably
race has been excluded). While overall, “Northpointe’s assessment tool
correctly predicts recidivism 61 percent of the time…blacks are almost
twice as likely as whites to be labeled a higher risk but not actually
re-offend. It makes the opposite mistake among whites: They are much
more likely than blacks to be labeled lower risk but go on to commit
other crimes. (Source: <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" class="external-link">ProPublica
analysis of data from Broward County, Fla.</a>)”. Interestingly, <a href="https://www.theatlantic.com/technology/archive/2018/01/equivant-compas-algorithm/550646/" class="external-link">a
subsequent study</a> has revealed that the COMPAS tool was actually no
better at predicting crimes than random people on the internet.</p>
<p>Bias may exist in other forms, such as an image dataset that
inadvertently contains objects that always happen to appear in the
center of the image, making it hard for a classifier to recognise
objects that are not in the center of images.</p>
<p>Bias may even be introduced to an algorithm in order to correct an
unfair model. Consider an image search algorithm, based on real world
data where a majority of men are CEOs. A user searching for “CEO” will
find images of primarily men, and a good bias may be introduced in order
to insure proper representation of a diversity of CEOs.</p>
<p>In whatever form it takes, and we’ll cover more of these forms in
this episode, it is crucial for model builders to be able to identify
bias and manage it.</p>
<blockquote>
<p>“Although neural networks might be said to write their own programs,
they do so towards goals set by humans, using data collected for human
purposes. If the data is skewed, even by accident, the computers will
amplify injustice.” — The Guardian</p>
</blockquote>
<div id="activity" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="activity" class="callout-inner">
<h3 class="callout-title">Activity</h3>
<div class="callout-content">
<p>Consider this riddle: A father and son get in a car crash and are
rushed to the hospital. The father dies. The boy is taken to the
operating room and the surgeon says, “I can’t operate on this boy,
because he’s my son.” How is this possible?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>The surgeon is a woman. In research conducted on 197 BU psychology
students (where women outnumbered men two-to-one) and 103 children, ages
7 to 17, only a small minority of subjects—15 percent of the children
and 14 percent of the BU students—came up with this answer. Of
self-described feminists in the student group, a majority, 78 percent,
did not guess the surgeon was the mother. This illustrates how a
training dataset may unintentionally come to encode societal gender bias
through human applied labels such as “Surgeon” vs. “Female Surgeon”.</p>
</div>
</div>
</div>
</div>
<div id="note" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="note" class="callout-inner">
<h3 class="callout-title">Note</h3>
<div class="callout-content">
<p>Data bias here is not to be confused with the <a href="https://developers.google.com/machine-learning/glossary#bias-math" class="external-link">“bias
term”</a> also used in machine learning or statistical terms such as <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" class="external-link">bias-variance
trade-off</a>.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="common-bias-types-in-machine-learning">Common bias types in machine learning<a class="anchor" aria-label="anchor" href="#common-bias-types-in-machine-learning"></a></h2>
<hr class="half-width"><p>Let’s take a closer look at some specific and common types of bias
that may manifest in the undertaking of machine learning approaches at
your institution. This is of course only a small handful of potential
sources of bias that may affect our judgment and skew a model’s
predictions. It’s important for model builders to be vigilant about
finding and remedying bias in whatever form it may enter the machine
learning pipeline.</p>
<table class="table"><colgroup><col width="1%"><col width="27%"><col width="70%"></colgroup><thead><tr class="header"><th>Type</th>
<th>Definition</th>
<th>Example</th>
</tr></thead><tbody><tr class="odd"><td>Prejudice bias</td>
<td>Arises when data incorporates cultural, race, gender or stereotypes
it should be ignorant of</td>
<td>A model is designed to differentiate between men and women in a
museum’s photograph collection. The training data contains more pictures
of women in kitchens than men in kitchens, or more pictures of men
coding than women, then the algorithm is trained to make incorrect
inferences about the gender of people engaged in those activities due to
prejudices that occur in the real world, represented in the data.</td>
</tr><tr class="even"><td>Selection bias</td>
<td>Introduced by the selection of individuals, groups or data for
analysis in such a way that proper randomization is not achieved,
thereby ensuring that the sample obtained is not representative of the
population intended to be analyzed</td>
<td>A model is trained to predict future sales of a new product line for
the museum gift shop. To build the training set, the first 200
subscribers to the museum’s newsletter were offered a small gift voucher
to fill in a survey. Instead of randomly targeting consumers, the
dataset targets newsletter subscribers who don’t necessarily represent
the museum’s potential paying customers. It’s entirely possible the
newsletter subscribers population may be more inclined to be signed up
to learn about free events and giveaways, while typical consumers may
not be enticed by small gift vouchers or even signed up at all.</td>
</tr><tr class="odd"><td>Confirmation bias</td>
<td>In the process of refining and reinforcing a models learning,
unconsciously or consciously processing data in ways that confirm
preexisting beliefs and hypotheses.</td>
<td>An engineer is building a model that predicts aggressiveness in dogs
based on a variety of features (height, weight, breed, environment). The
engineer had an unpleasant encounter with a hyperactive toy poodle as a
child, and ever since has associated the breed with aggression. When the
trained model predicted most toy poodles to be relatively docile, the
engineer retrained the model several more times until it produced a
result showing smaller poodles to be more violent.</td>
</tr><tr class="even"><td>Correlation bias</td>
<td>Correlation is not causation. Correlation implies the mutual
relation, covariation, or association between two or more variables. It
only questions whether the variable varies together or not.</td>
<td>The height of the father and his children is correlated, but one
can’t say that the father’s height is caused by determining his
children’s height on the only assumption of the hereditary factor. There
are several other factors present such as environment, genetics,
etc.</td>
</tr><tr class="odd"><td>Exclusion bias</td>
<td>Removing data from a set that we think isn’t relevant</td>
<td>For example, imagine you have a dataset of customer sales in America
and Canada. 98% of the customers are from America, so you choose to
delete the location data thinking it is irrelevant. However, this means
your model will not pick up on the fact that your Canadian customers
spend two times more.</td>
</tr></tbody></table></section><section><h2 class="section-heading" id="when-might-human-bias-enter-a-machine-learning-pipeline">When might human bias enter a machine learning pipeline?<a class="anchor" aria-label="anchor" href="#when-might-human-bias-enter-a-machine-learning-pipeline"></a></h2>
<hr class="half-width"><p>There are abundant opportunities for human bias to enter ML systems
at all stages of the pipeline including:</p>
<ul><li>When the study is being designed</li>
<li>When datasets are constructed</li>
<li>When decisions are being made to refine and reinforce a models
learning</li>
<li>When interpreting and applying decisions made by the model to real
world scenarios</li>
</ul><div class="section level3">
<h3 id="bias-arising-in-the-study-design">Bias arising in the study design<a class="anchor" aria-label="anchor" href="#bias-arising-in-the-study-design"></a></h3>
<p>Some machine learning systems are quite simply built on ethically
unsound foundations from the outset. A recent controversial study, <a href="https://www.inputmag.com/culture/this-algorithmic-study-about-trustworthiness-has-some-glaring-flaws" class="external-link">Tracking
historical changes in trustworthiness using machine learning analyses of
facial cues in paintings</a>, published in Nature Communications,
garnered significant controversy for its proximity to the thoroughly
debunked pseudoscience <a href="https://en.wikipedia.org/wiki/Phrenology" class="external-link">phrenology</a> which
aims to assess an individuals personality and (or in the case of this
study, trust) based on facial structure.</p>
</div>
<div class="section level3">
<h3 id="bias-arising-in-dataset-collection-and-construction">Bias arising in dataset collection and construction<a class="anchor" aria-label="anchor" href="#bias-arising-in-dataset-collection-and-construction"></a></h3>
<blockquote>
<p>“It’s all too easy to forget that data is about human beings and
their behaviors. Data is not an abstraction…Data encodes the stories of
our lives, capturing not only our tastes and interests but also our
hopes and fears. Data isn’t an abstract idea or a set of numbers or
qualitative responses. It can be and is, ultimately, human.
(reference)”</p>
</blockquote>
<p>Data is never neutral. It is collected, managed, and organised by and
about people. And the manner in which data is sourced and constructed
for training sets will have important implications on your model’s
output. In the simplest example, if you are aiming to build a face
recognition software but only train your model on a dataset of white
faces, your model is obviously going to struggle to identify faces of
any other colour.</p>
<p>Because of the many complexities around copyright and licensing,
privacy issues and high costs involved in getting access to large
quantities of quality datasets, data scientists have favoured scraping
what they can find freely, en masse and indiscriminately, across the
internet from free sources such as <a href="https://aws.amazon.com/de/datasets/wikipedia-xml-data/" class="external-link">Wikipedia</a>,
FlickR or Google News. Datasets collected in this manner, taken without
intervention, will reflect the biases of these sources, such as a
certain demographic composition (overrepresentation of young internet
users from developed countries for example).</p>
<p>When training data is labelled (annotated), typically through some
kind of crowdsourcing mechanism such as Amazon’s Mechanical Turk, and as
we saw in the first activity, bias can crop up, either consciously or
unconsciously, in the course of this. Whether personally undertaking or
crowdsourcing your data annotation, it’s important to be aware of how
different demographics and social constructs may introduce bias and
implicit associations into your pipeline.</p>
<p>GLAM staff will be more than familiar with this phenomenon as we
grapple with <a href="https://cataloginglab.org/list-of-statements-on-bias-in-library-and-archives-description/" class="external-link">historical
bias in library and archives descriptions</a>. As demand for GLAM
collections and catalogue data for use in machine learning increases, it
is vital that model builders are made aware of the biases that may be
encoded within cultural heritage data.</p>
<figure><img src="fig/ep-04-Google_Inclusive_Activity_1.png" alt="Four separate images depicting wedding ceremonies" class="figure mx-auto d-block"></figure><div id="activity-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="activity-1" class="callout-inner">
<h3 class="callout-title">Activity</h3>
<div class="callout-content">
<p>Consider this image and write a list of terms you would use to
annotate it. Compare your outputs with your nearest neighbour(s).
Discuss the differences and how this could effect a model. How might you
mitigate these differences in annotations?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Explore Further </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p><img src="fig/ep-04-Google_Inclusive_Activity_2.png" alt="Four separate images depicting wedding ceremonies with annotations" class="figure">
The image comes from wedding photographs (donated by Googlers) for the
<a href="https://ai.googleblog.com/2018/09/introducing-inclusive-images-competition.html" class="external-link">Open
Images dataset</a>. How well did you do compared to a classifier’s label
predictions (recorded below each image).</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="bias-arising-when-refining-and-reinforcing-a-models-learning">Bias arising when refining and reinforcing a models learning<a class="anchor" aria-label="anchor" href="#bias-arising-when-refining-and-reinforcing-a-models-learning"></a></h3>
<p>We’ve talked a lot about bias that can make it into our training
data, but this isn’t the only way it manifests itself in machine
learning systems. As we now know from earlier in this lesson, machine
learning models are refined and reinforced based on reactions to its
results. In this process, there is a risk of certain outcomes being
ignored and others privileged over others, skewing a models
learning.</p>
<p>For example, A model builder is using named entity recognition across
multilingual newspapers. They might determine they are satisfied when
the model gets to 90% accuracy and will aim to improve to this result.
However this overall accuracy can hide the fact that some particular
‘slices’ of our data might have much worse accuracy. Your overall
accuracy might be very good but your model may underperform on one
language. This might not be addressed by changing your data but changing
how you approach training and evaluating your model.</p>
</div>
<div class="section level3">
<h3 id="bias-arising-in-the-application-of-machine-learning-decisions-to-real-world-scenarios">Bias arising in the application of machine learning decisions to
real world scenarios<a class="anchor" aria-label="anchor" href="#bias-arising-in-the-application-of-machine-learning-decisions-to-real-world-scenarios"></a></h3>
<p>This may be an area you might already have some awareness of as the
ubiquity of machine learning systems in our lives increases, the effects
of unfair, biased algorithms are starting to become more apparent.
Algorithmic bias is defined as unjust, unfair, or prejudicial treatment
of people related to race, income, sexual orientation, religion, gender,
and other characteristics historically associated with discrimination
and marginalization, when and where they manifest in algorithmic systems
and algorithmically aided decision-making. Predictive policing as
covered earlier is probably the most clear example of algorithmic bias.
#TODO: add Amazon hiring example. Another canonical example of this is
the Amazon hiring</p>
<p>It is imperative to be aware then that the manner in which data is
collected, annotated and results applied, will have far reaching
consequences for society as decisions produced by ML systems are
increasingly being relied upon in real world scenarios. From seemingly
benign systems like recommendation engines to predictive policing,
opportunities for ML systems to perpetuate and amplify humans bias and
inequality abound.</p>
</div>
</section><section><h2 class="section-heading" id="how-can-glam-staff-help-manage-bias-in-machine-learning-approaches">How can GLAM staff help manage bias in machine learning
approaches?<a class="anchor" aria-label="anchor" href="#how-can-glam-staff-help-manage-bias-in-machine-learning-approaches"></a></h2>
<hr class="half-width"><blockquote>
<p>Archives are the longest standing communal effort to gather human
information and archive scholars have already developed the language and
procedures to address and discuss many challenges pertaining to data
collection such as consent, power, inclusivity, transparency, and ethics
&amp; privacy. <a href="https://dl.acm.org/doi/pdf/10.1145/3351095.3372829" class="external-link">Reference
Lessons from archives: strategies for collecting sociocultural data in
machine learning</a></p>
</blockquote>
<p>The presence of bias in the classifications and predictions of
machine learning is a key challenge today, but being aware of and
transparent about the problem allows us to take proactive steps to
mitigate their effects. For GLAM professionals, this is familiar ground
and in <a href="https://dl.acm.org/doi/pdf/10.1145/3351095.3372829" class="external-link">Reference
Lessons from archives: strategies for collecting sociocultural data in
machine learning</a> the authors argue that the document collection
practices in archives present the ideal ethical and practical framework
for mitigating bias in data collection for the field of machine
learning. GLAM professionals have an opportunity to apply these tools to
the management of bias.</p>
<p>“That means,” said Catanzaro, “that the collection and curation of
data sets, the skills that you guys practice—the resources that you have
access to—can enable the creation of new algorithms and new
applications.”(Catanzaro, 2019, 5:30)</p>
<p>How else might GLAM staff actively help manage bias in machine
learning approaches?</p>
<ul><li>GLAM institutions must ensure a diverse workforce-monoculture cannot
effectively manage bias-diversity is not an option, it is an imperative
<a href="https://www.oclc.org/content/dam/research/publications/2019/oclcresearch-responsible-operations-data-science-machine-learning-ai-a4.pdf" class="external-link">(Responsible
Operations: Data Science, Machine Learning, and AI in
Libraries)</a>)</li>
<li>Hold symposia focused on surfacing historic and contemporary
approaches to managing bias with an explicit social and technical
focusing on the challenges libraries faced in managing bias while
adopting technologies like computation, the internet, and currently with
data science, machine learning, and AI.</li>
<li>Contribute diverse language materials, collections and texts to
sources where model builders are finding their data such as
Wikipedia</li>
<li>Collaborate directly with computer scientists to build new diverse,
curated data sets for use in machine learning (<a href="https://www.bl.uk/projects/arabic-htr" class="external-link">Arabic HTR</a>)</li>
<li>Enlist the help of staff with the right domain expertise to review
training data construction before and after, they may see biases that
you have overlooked</li>
<li>When collecting and annotating data make sure to recruit diversified
crowds for the task and carefully communicate instructions</li>
<li>Employ toolkits for detecting and removing bias from machine
learning models, for instance the IBM open sourced <a href="https://aif360.mybluemix.net/" class="external-link">AI fairness 360 tool</a>
</li>
<li>Consider your partnerships and collaborators closely, including
ramifications of outsourcing your AI to external companies/partners</li>
<li>Know your data and your model and be transparent about it. Create <a href="https://arxiv.org/abs/1803.09010v3" class="external-link">Datasheets</a> and <a href="https://arxiv.org/abs/1810.03993" class="external-link">Model cards</a> describing and
documenting them in full for users</li>
</ul><div id="activity-2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="activity-2" class="callout-inner">
<h3 class="callout-title">Activity</h3>
<div class="callout-content">
<p>In small groups, consider the following potential machine learning
project. Discuss 2-3 potential points at which bias may enter the
pipeline, and questions/strategies GLAM staff might want to consider in
order to manage it.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Discuss </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p><em>A museum is keen to make a newly acquired digitised collection of
20,000 Southeast Asian late 19th century photographs more discoverable
within the main catalogue. The photographs are the work of an English
traveler, and aside from captions handwritten by him, the individual
photographs have very little in the way of item level description. A
model will be trained to help classify the photographs (places, people,
events, objects) and the new machine generated descriptive tags added to
the catalogue records. To create the training set from which the model
will learn, a crowdsourcing project will bet set-up asking members of
the public to add their own descriptive tags to a subset of the
images.</em></p>
</div>
</div>
</div>
</div>
<blockquote>
<h2 id="resources-consulted-recommended-reading">Resources Consulted
&amp; Recommended Reading</h2>
<ul><li>Barbosa, N., &amp; Chen, M. (2021). Rehumanized Crowdsourcing.
Proceedings of the 2019 CHI Conference on Human Factors in Computing
Systems. Dl.acm.org. Retrieved 29 March 2021, from <a href="https://dl.acm.org/doi/10.1145/3290605.3300773" class="external-link uri">https://dl.acm.org/doi/10.1145/3290605.3300773</a>.</li>
<li>Barlow, R. (2014). BU Research: A Riddle Reveals Depth of Gender
Bias. BU Today. Boston University. Retrieved 29 March 2021, from <a href="https://www.bu.edu/articles/2014/bu-research-riddle-reveals-the-depth-of-gender-bias" class="external-link uri">https://www.bu.edu/articles/2014/bu-research-riddle-reveals-the-depth-of-gender-bias</a>.</li>
<li>Catanzaro, B. (2019, December 4). “Datasets make algorithms: how
creating, curating, and distributing data creates modern AI.” [Video
file]. Retrieved from <a href="https://library.stanford.edu/projects/fantastic-futures" class="external-link uri">https://library.stanford.edu/projects/fantastic-futures</a>.</li>
<li>Coleman, C. (2020). Managing Bias When Library Collections Become
Data. International Journal Of Librarianship, 5(1), 8-19. <a href="https://doi.org/10.23974/ijol.2020.vol5.1.162" class="external-link uri">https://doi.org/10.23974/ijol.2020.vol5.1.162</a>.</li>
<li>Ekowo, M. (2016). Why Numbers can be Neutral but Data Can’t. New
America. Retrieved 29 March 2021, from <a href="https://www.newamerica.org/education-policy/edcentral/numbers-can-neutral-data-cant/" class="external-link uri">https://www.newamerica.org/education-policy/edcentral/numbers-can-neutral-data-cant/</a>.</li>
<li>Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J., Wallach, H.,
Daumeé III, H., &amp; Crawford, K. (2020). Datasheets for Datasets.
arXiv.org. Retrieved 29 March 2021, from <a href="https://arxiv.org/abs/1803.09010v3" class="external-link uri">https://arxiv.org/abs/1803.09010v3</a>.</li>
<li>Hellström, T., Dignum, V., &amp; Bensch, S. (2020). Bias in Machine
Learning What is it Good (and Bad) for?. arXiv preprint. Retrieved 20
April 2021, from <a href="https://arxiv.org/abs/2004.00686v2" class="external-link uri">https://arxiv.org/abs/2004.00686v2</a>.</li>
<li>Jo, E., &amp; Gebru, T. (2020). Lessons from archives. Proceedings
Of The 2020 Conference On Fairness, Accountability, And Transparency. <a href="https://doi.org/10.1145/3351095.3372829" class="external-link uri">https://doi.org/10.1145/3351095.3372829</a>.</li>
<li>Mayson, Sandra Gabriel, Bias In, Bias Out (2019). 128 Yale Law
Journal 2218, University of Georgia School of Law Legal Studies Research
Paper No. 2018-35, Available at SSRN: <a href="https://ssrn.com/abstract=3257004" class="external-link uri">https://ssrn.com/abstract=3257004</a>.</li>
<li>Padilla, T. (2019). Responsible Operations: Data Science, Machine
Learning, and AI in Libraries. OCLC Research Position Paper. <a href="https://doi.org/10.25333/xk7z-9g97" class="external-link uri">https://doi.org/10.25333/xk7z-9g97</a>.</li>
</ul></blockquote>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul><li>Bias occurs when a dataset is not representative of the population,
it is incomplete or skewed.</li>
<li>The presence of bias in the classifications and predictions of
machine learning may have far reaching consequences for society,
amplifying inequality and unfairness.</li>
<li>There are abundant opportunities for bias to enter ML systems at all
stages of the pipeline including when datasets are constructed, when a
models learning is refined and reinforced, and when predictions made by
a model are interpreted by humans and applied to real world
scenarios</li>
<li>There are a range of strategies available today to help mitigate
bias.</li>
</ul></div>
</div>
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="04-what-is-ml-good-at.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="06-applying-machine-learning.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="04-what-is-ml-good-at.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: What is Machine
        </a>
        <a class="chapter-link float-end" href="06-applying-machine-learning.html" rel="next">
          Next: Applying Machine...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries-incubator/machine-learning-librarians-archivists/edit/main/episodes/05-managing-data-bias.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries-incubator/machine-learning-librarians-archivists/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/machine-learning-librarians-archivists/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/machine-learning-librarians-archivists/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.10" class="external-link">sandpaper (0.16.10)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.7" class="external-link">pegboard (0.7.7)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.5" class="external-link">varnish (1.0.5)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries-incubator.github.io/machine-learning-librarians-archivists/05-managing-data-bias.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "Understanding and managing bias",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/machine-learning-librarians-archivists/05-managing-data-bias.html",
  "identifier": "https://carpentries-incubator.github.io/machine-learning-librarians-archivists/05-managing-data-bias.html",
  "dateCreated": "2021-02-11",
  "dateModified": "2024-11-14",
  "datePublished": "2024-11-14"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

